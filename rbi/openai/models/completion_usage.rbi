# typed: strong

module OpenAI
  module Models
    class CompletionUsage < OpenAI::Internal::Type::BaseModel
      OrHash = T.type_alias { T.any(T.self_type, OpenAI::Internal::AnyHash) }

      # Number of tokens in the generated completion.
      sig { returns(Integer) }
      attr_accessor :completion_tokens

      # Number of tokens in the prompt.
      sig { returns(Integer) }
      attr_accessor :prompt_tokens

      # Total number of tokens used in the request (prompt + completion).
      sig { returns(Integer) }
      attr_accessor :total_tokens

      # Breakdown of tokens used in a completion.
      sig do
        returns(T.nilable(OpenAI::CompletionUsage::CompletionTokensDetails))
      end
      attr_reader :completion_tokens_details

      sig do
        params(
          completion_tokens_details:
            OpenAI::CompletionUsage::CompletionTokensDetails::OrHash
        ).void
      end
      attr_writer :completion_tokens_details

      # Breakdown of tokens used in the prompt.
      sig { returns(T.nilable(OpenAI::CompletionUsage::PromptTokensDetails)) }
      attr_reader :prompt_tokens_details

      sig do
        params(
          prompt_tokens_details:
            OpenAI::CompletionUsage::PromptTokensDetails::OrHash
        ).void
      end
      attr_writer :prompt_tokens_details

      # Usage statistics for the completion request.
      sig do
        params(
          completion_tokens: Integer,
          prompt_tokens: Integer,
          total_tokens: Integer,
          completion_tokens_details:
            OpenAI::CompletionUsage::CompletionTokensDetails::OrHash,
          prompt_tokens_details:
            OpenAI::CompletionUsage::PromptTokensDetails::OrHash
        ).returns(T.attached_class)
      end
      def self.new(
        # Number of tokens in the generated completion.
        completion_tokens:,
        # Number of tokens in the prompt.
        prompt_tokens:,
        # Total number of tokens used in the request (prompt + completion).
        total_tokens:,
        # Breakdown of tokens used in a completion.
        completion_tokens_details: nil,
        # Breakdown of tokens used in the prompt.
        prompt_tokens_details: nil
      )
      end

      sig do
        override.returns(
          {
            completion_tokens: Integer,
            prompt_tokens: Integer,
            total_tokens: Integer,
            completion_tokens_details:
              OpenAI::CompletionUsage::CompletionTokensDetails,
            prompt_tokens_details: OpenAI::CompletionUsage::PromptTokensDetails
          }
        )
      end
      def to_hash
      end

      class CompletionTokensDetails < OpenAI::Internal::Type::BaseModel
        OrHash = T.type_alias { T.any(T.self_type, OpenAI::Internal::AnyHash) }

        # When using Predicted Outputs, the number of tokens in the prediction that
        # appeared in the completion.
        sig { returns(T.nilable(Integer)) }
        attr_reader :accepted_prediction_tokens

        sig { params(accepted_prediction_tokens: Integer).void }
        attr_writer :accepted_prediction_tokens

        # Audio input tokens generated by the model.
        sig { returns(T.nilable(Integer)) }
        attr_reader :audio_tokens

        sig { params(audio_tokens: Integer).void }
        attr_writer :audio_tokens

        # Tokens generated by the model for reasoning.
        sig { returns(T.nilable(Integer)) }
        attr_reader :reasoning_tokens

        sig { params(reasoning_tokens: Integer).void }
        attr_writer :reasoning_tokens

        # When using Predicted Outputs, the number of tokens in the prediction that did
        # not appear in the completion. However, like reasoning tokens, these tokens are
        # still counted in the total completion tokens for purposes of billing, output,
        # and context window limits.
        sig { returns(T.nilable(Integer)) }
        attr_reader :rejected_prediction_tokens

        sig { params(rejected_prediction_tokens: Integer).void }
        attr_writer :rejected_prediction_tokens

        # Breakdown of tokens used in a completion.
        sig do
          params(
            accepted_prediction_tokens: Integer,
            audio_tokens: Integer,
            reasoning_tokens: Integer,
            rejected_prediction_tokens: Integer
          ).returns(T.attached_class)
        end
        def self.new(
          # When using Predicted Outputs, the number of tokens in the prediction that
          # appeared in the completion.
          accepted_prediction_tokens: nil,
          # Audio input tokens generated by the model.
          audio_tokens: nil,
          # Tokens generated by the model for reasoning.
          reasoning_tokens: nil,
          # When using Predicted Outputs, the number of tokens in the prediction that did
          # not appear in the completion. However, like reasoning tokens, these tokens are
          # still counted in the total completion tokens for purposes of billing, output,
          # and context window limits.
          rejected_prediction_tokens: nil
        )
        end

        sig do
          override.returns(
            {
              accepted_prediction_tokens: Integer,
              audio_tokens: Integer,
              reasoning_tokens: Integer,
              rejected_prediction_tokens: Integer
            }
          )
        end
        def to_hash
        end
      end

      class PromptTokensDetails < OpenAI::Internal::Type::BaseModel
        OrHash = T.type_alias { T.any(T.self_type, OpenAI::Internal::AnyHash) }

        # Audio input tokens present in the prompt.
        sig { returns(T.nilable(Integer)) }
        attr_reader :audio_tokens

        sig { params(audio_tokens: Integer).void }
        attr_writer :audio_tokens

        # Cached tokens present in the prompt.
        sig { returns(T.nilable(Integer)) }
        attr_reader :cached_tokens

        sig { params(cached_tokens: Integer).void }
        attr_writer :cached_tokens

        # Breakdown of tokens used in the prompt.
        sig do
          params(audio_tokens: Integer, cached_tokens: Integer).returns(
            T.attached_class
          )
        end
        def self.new(
          # Audio input tokens present in the prompt.
          audio_tokens: nil,
          # Cached tokens present in the prompt.
          cached_tokens: nil
        )
        end

        sig do
          override.returns({ audio_tokens: Integer, cached_tokens: Integer })
        end
        def to_hash
        end
      end
    end
  end
end
